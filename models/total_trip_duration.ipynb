{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ed0ef3",
   "metadata": {},
   "source": [
    "# Promedios de ETA a próxima estación por línea y dirección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c8883",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "FEATS_DIR = Path(\"features_ready\")\n",
    "files = sorted(FEATS_DIR.glob(\"*.parquet\"))\n",
    "assert files, \"No hay .parquet\"\n",
    "\n",
    "# -------------------------\n",
    "# Parámetros de robustez\n",
    "# -------------------------\n",
    "D_IN_M = 30.0      # umbral de llegada por distancia\n",
    "T_IN_S = 20.0      # umbral alterno por ETA\n",
    "V_SLOW_MPS = 2.5   # opcional: \"llegada\" con velocidad baja\n",
    "USE_SLOW_VEL = False  # pon True si quieres exigir velocidad baja\n",
    "\n",
    "# -------------------------\n",
    "# Utilidades\n",
    "# -------------------------\n",
    "def _runs_of_equal(values: pd.Series):\n",
    "    v = values.astype(\"string\").fillna(\"<NA>\").values\n",
    "    n = len(v)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    start = np.empty(n, dtype=bool); start[0] = True\n",
    "    start[1:] = v[1:] != v[:-1]\n",
    "    rid = np.cumsum(start) - 1\n",
    "    out = []\n",
    "    for r in np.unique(rid):\n",
    "        idx = np.where(rid == r)[0]\n",
    "        out.append((int(idx[0]), int(idx[-1]), v[idx[0]]))\n",
    "    return out  # lista de (i_ini, i_fin, nombre_estacion_objetivo)\n",
    "\n",
    "def _detect_arrival_idx(block: pd.DataFrame) -> int | None:\n",
    "    # candidato: índice con mínima distancia\n",
    "    i_min = int(block[\"dist_a_prox_m\"].idxmin())\n",
    "    row = block.loc[i_min]\n",
    "    ok_dist = row.get(\"dist_a_prox_m\", np.inf) <= D_IN_M\n",
    "    ok_eta  = row.get(\"ETA_prox_est_s\", np.inf) <= T_IN_S\n",
    "    ok_vel  = (row.get(\"vel_mps\", np.nan) <= V_SLOW_MPS) if USE_SLOW_VEL else True\n",
    "    if (ok_dist or ok_eta) and ok_vel:\n",
    "        return i_min\n",
    "    # fallback: usa el último registro del bloque\n",
    "    return int(block.index[-1])\n",
    "\n",
    "def _first_move_after(df: pd.DataFrame, idx_after: int) -> int | None:\n",
    "    # Busca el primer índice > idx_after con \"progreso\"\n",
    "    # Si no tienes progress_event/is_no_progress, usa cambio de posición o vel_mps> threshold\n",
    "    if \"progress_event\" in df.columns:\n",
    "        s = df.loc[idx_after+1:, \"progress_event\"]\n",
    "        got = s.index[s.values == 1]\n",
    "        return int(got[0]) if len(got) else None\n",
    "    elif \"is_no_progress\" in df.columns:\n",
    "        s = df.loc[idx_after+1:, \"is_no_progress\"]\n",
    "        got = s.index[s.values == 0]\n",
    "        return int(got[0]) if len(got) else None\n",
    "    else:\n",
    "        # heurística: vel_mps > 0.8 m/s\n",
    "        if \"vel_mps\" in df.columns:\n",
    "            s = df.loc[idx_after+1:, \"vel_mps\"]\n",
    "            got = s.index[s.values > 0.8]\n",
    "            return int(got[0]) if len(got) else None\n",
    "        return None\n",
    "\n",
    "def _clip_outliers_iqr(x: pd.Series):\n",
    "    q1, q3 = x.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    return x[(x >= lo) & (x <= hi)], lo, hi\n",
    "\n",
    "# -------------------------\n",
    "# Proceso principal\n",
    "# -------------------------\n",
    "all_rows = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_parquet(f)\n",
    "\n",
    "    # Normalizaciones mínimas\n",
    "    if \"Fecha\" in df.columns:\n",
    "        df[\"Fecha\"] = pd.to_datetime(df[\"Fecha\"])\n",
    "    if \"vel_mps\" not in df.columns and \"Velocidad (km/h)\" in df.columns:\n",
    "        df[\"vel_mps\"] = df[\"Velocidad (km/h)\"] / 3.6\n",
    "\n",
    "    # Orden crítico\n",
    "    df = df.sort_values([\"LINEA\",\"DIR\",\"Placa\",\"trip_id\",\"Fecha\"]).reset_index(drop=True)\n",
    "\n",
    "    # Recorre por viaje\n",
    "    for (linea, direc, placa, trip), g in df.groupby([\"LINEA\",\"DIR\",\"Placa\",\"trip_id\"], sort=False):\n",
    "        if g.empty: \n",
    "            continue\n",
    "        g = g.copy()\n",
    "\n",
    "        # Construye runs por proxima_est_teorica\n",
    "        runs = _runs_of_equal(g[\"proxima_est_teorica\"])\n",
    "        if len(runs) < 2:\n",
    "            continue  # no hay siguiente estación\n",
    "\n",
    "        # Detecta llegadas por bloque y arma pares consecutivos\n",
    "        arrivals = []  # [(estacion, idx_arr, ts_arr)]\n",
    "        for (i0, i1, est) in runs:\n",
    "            blk = g.iloc[i0:i1+1]\n",
    "            if est == \"<NA>\":\n",
    "                continue\n",
    "            idx_arr = _detect_arrival_idx(blk)\n",
    "            ts_arr  = g.loc[idx_arr, \"Fecha\"]\n",
    "            arrivals.append((str(est), idx_arr, ts_arr))\n",
    "\n",
    "        # Pares (S_j -> S_{j+1})\n",
    "        for j in range(len(arrivals)-1):\n",
    "            est_j, idx_arr_j, t_arr_j = arrivals[j]\n",
    "            est_k, idx_arr_k, t_arr_k = arrivals[j+1]\n",
    "\n",
    "            if pd.isna(t_arr_j) or pd.isna(t_arr_k) or t_arr_k <= t_arr_j:\n",
    "                continue\n",
    "\n",
    "            # A→A\n",
    "            aa_s = (t_arr_k - t_arr_j).total_seconds()\n",
    "\n",
    "            # D→A (opcional)\n",
    "            idx_dep = _first_move_after(g, idx_arr_j)\n",
    "            da_s = None\n",
    "            if idx_dep is not None:\n",
    "                t_dep = g.loc[idx_dep, \"Fecha\"]\n",
    "                if t_dep < t_arr_k:\n",
    "                    da_s = (t_arr_k - t_dep).total_seconds()\n",
    "\n",
    "            all_rows.append({\n",
    "                \"LINEA\": linea,\n",
    "                \"DIR\": direc,\n",
    "                \"estacion_actual\": est_j,\n",
    "                \"siguiente_estacion\": est_k,\n",
    "                \"trip_id\": str(trip),\n",
    "                \"A2A_s\": aa_s,\n",
    "                \"D2A_s\": da_s,\n",
    "                \"archivo\": f.name\n",
    "            })\n",
    "\n",
    "# Tabla cruda de observaciones por tramo\n",
    "obs = pd.DataFrame(all_rows)\n",
    "if obs.empty:\n",
    "    raise SystemExit(\"No se generaron observaciones de tramos (revisa columnas y datos)\")\n",
    "\n",
    "def robust_agg(x: pd.Series, colname: str):\n",
    "    x = x.dropna()\n",
    "    n_total = len(x)\n",
    "    if n_total == 0:\n",
    "        return pd.Series({\n",
    "            f\"{colname}_n\": 0,\n",
    "            f\"{colname}_p50\": np.nan,\n",
    "            f\"{colname}_p10\": np.nan,\n",
    "            f\"{colname}_p25\": np.nan,\n",
    "            f\"{colname}_p75\": np.nan,\n",
    "            f\"{colname}_p90\": np.nan,\n",
    "            f\"{colname}_mean_trim10\": np.nan,\n",
    "            f\"{colname}_std\": np.nan,\n",
    "            f\"{colname}_discard_ratio\": np.nan\n",
    "        })\n",
    "    x_f, lo, hi = _clip_outliers_iqr(x)\n",
    "    discard_ratio = 1 - (len(x_f) / n_total)\n",
    "    if len(x_f) == 0:\n",
    "        x_f = x  # si filtró todo, usa sin filtrar pero lo sabrás por discard_ratio=1\n",
    "    # media recortada 10%\n",
    "    xf_sorted = x_f.sort_values().values\n",
    "    k = int(0.10*len(xf_sorted))\n",
    "    if len(xf_sorted) - 2*k > 0:\n",
    "        mean_trim10 = xf_sorted[k:len(xf_sorted)-k].mean()\n",
    "    else:\n",
    "        mean_trim10 = x_f.mean()\n",
    "\n",
    "    return pd.Series({\n",
    "        f\"{colname}_n\": n_total,\n",
    "        f\"{colname}_p50\": x_f.median(),\n",
    "        f\"{colname}_p10\": x_f.quantile(0.10),\n",
    "        f\"{colname}_p25\": x_f.quantile(0.25),\n",
    "        f\"{colname}_p75\": x_f.quantile(0.75),\n",
    "        f\"{colname}_p90\": x_f.quantile(0.90),\n",
    "        f\"{colname}_mean_trim10\": mean_trim10,\n",
    "        f\"{colname}_std\": x_f.std(ddof=1),\n",
    "        f\"{colname}_discard_ratio\": discard_ratio\n",
    "    })\n",
    "\n",
    "# Agregación robusta por tramo\n",
    "grp = [\"LINEA\",\"DIR\",\"estacion_actual\",\"siguiente_estacion\"]\n",
    "summary = (obs\n",
    "    .groupby(grp, as_index=False)\n",
    "    .apply(lambda g: pd.concat([robust_agg(g[\"A2A_s\"], \"A2A\"),\n",
    "                                robust_agg(g[\"D2A_s\"], \"D2A\")], axis=0))\n",
    "    .reset_index()\n",
    "    .drop(columns=[\"level_0\",\"level_1\"], errors=\"ignore\")\n",
    ")\n",
    "\n",
    "# Recomendación como ETA \"confiable\": usa la mediana A2A o D2A si quieres excluir dwell\n",
    "summary[\"ETA_sugerido_s\"] = np.where(\n",
    "    summary[\"D2A_p50\"].notna(), summary[\"D2A_p50\"], summary[\"A2A_p50\"]\n",
    ")\n",
    "\n",
    "# (Opcional) ordena por línea y dirección\n",
    "summary = summary.sort_values(grp).reset_index(drop=True)\n",
    "\n",
    "# ¡listo! 'summary' es tu tabla de ETA por tramo\n",
    "summary.head(15)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
