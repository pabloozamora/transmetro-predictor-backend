{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96846f28",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04167ffe",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcec0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from typing import Iterator, Tuple\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c977f04",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a89b724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Placa</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>block_id</th>\n",
       "      <th>LINEA</th>\n",
       "      <th>DIR</th>\n",
       "      <th>proxima_est_teorica</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>dist_a_prox_m</th>\n",
       "      <th>dist_estacion_m</th>\n",
       "      <th>vel_mps</th>\n",
       "      <th>...</th>\n",
       "      <th>dist_m</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>dwell_same_xy_s</th>\n",
       "      <th>is_no_progress</th>\n",
       "      <th>progress_event</th>\n",
       "      <th>hour</th>\n",
       "      <th>dow</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_peak</th>\n",
       "      <th>ETA_proxima_est_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Linea_12</td>\n",
       "      <td>IDA</td>\n",
       "      <td>MONTE MARÍA</td>\n",
       "      <td>2024-01-12 09:38:23</td>\n",
       "      <td>2507.478516</td>\n",
       "      <td>361.775177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>320.33194</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>657.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Linea_12</td>\n",
       "      <td>IDA</td>\n",
       "      <td>MONTE MARÍA</td>\n",
       "      <td>2024-01-12 09:39:23</td>\n",
       "      <td>2507.478516</td>\n",
       "      <td>361.775177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>320.33194</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>597.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Linea_12</td>\n",
       "      <td>IDA</td>\n",
       "      <td>MONTE MARÍA</td>\n",
       "      <td>2024-01-12 09:40:23</td>\n",
       "      <td>2507.478516</td>\n",
       "      <td>361.775177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>320.33194</td>\n",
       "      <td>60.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>537.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Linea_12</td>\n",
       "      <td>IDA</td>\n",
       "      <td>MONTE MARÍA</td>\n",
       "      <td>2024-01-12 09:41:23</td>\n",
       "      <td>2507.478516</td>\n",
       "      <td>361.775177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>320.33194</td>\n",
       "      <td>60.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Linea_12</td>\n",
       "      <td>IDA</td>\n",
       "      <td>MONTE MARÍA</td>\n",
       "      <td>2024-01-12 09:42:23</td>\n",
       "      <td>2507.478516</td>\n",
       "      <td>361.775177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>320.33194</td>\n",
       "      <td>60.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>417.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Placa trip_id block_id     LINEA  DIR proxima_est_teorica  \\\n",
       "0    49       1        1  Linea_12  IDA         MONTE MARÍA   \n",
       "1    49       1        1  Linea_12  IDA         MONTE MARÍA   \n",
       "2    49       1        1  Linea_12  IDA         MONTE MARÍA   \n",
       "3    49       1        1  Linea_12  IDA         MONTE MARÍA   \n",
       "4    49       1        1  Linea_12  IDA         MONTE MARÍA   \n",
       "\n",
       "                Fecha  dist_a_prox_m  dist_estacion_m  vel_mps  ...  \\\n",
       "0 2024-01-12 09:38:23    2507.478516       361.775177      0.0  ...   \n",
       "1 2024-01-12 09:39:23    2507.478516       361.775177      0.0  ...   \n",
       "2 2024-01-12 09:40:23    2507.478516       361.775177      0.0  ...   \n",
       "3 2024-01-12 09:41:23    2507.478516       361.775177      0.0  ...   \n",
       "4 2024-01-12 09:42:23    2507.478516       361.775177      0.0  ...   \n",
       "\n",
       "      dist_m  time_diff  dwell_same_xy_s  is_no_progress  progress_event  \\\n",
       "0  320.33194       60.0              0.0               0               0   \n",
       "1  320.33194       60.0             60.0               1               0   \n",
       "2  320.33194       60.0            120.0               1               0   \n",
       "3  320.33194       60.0            180.0               1               0   \n",
       "4  320.33194       60.0            240.0               1               0   \n",
       "\n",
       "   hour  dow  is_weekend  is_peak  ETA_proxima_est_s  \n",
       "0     9    4           0        1              657.0  \n",
       "1     9    4           0        1              597.0  \n",
       "2     9    4           0        1              537.0  \n",
       "3     9    4           0        1              477.0  \n",
       "4     9    4           0        1              417.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener .parquet de datos muestreados\n",
    "SAMPLE_PATH = 'D:/2025/UVG/Tesis/repos/backend/features_sampled_without_idle_rows/sample_features.parquet'\n",
    "\n",
    "data = pd.read_parquet(SAMPLE_PATH)\n",
    "\n",
    "# Renombrar columna \"Altitud (m)\" a \"Altitud\"\n",
    "data = data.rename(columns={\"Altitud (m)\": \"Altitud\"})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bdc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "\n",
    "# Columnas iniciales\n",
    "str_cols = [\"Placa\",\"trip_id\",\"block_id\",\"LINEA\",\"DIR\",\"proxima_est_teorica\"]\n",
    "dt_cols  = [\"Fecha\"]\n",
    "float_cols = [\n",
    "    \"dist_a_prox_m\",\"dist_estacion_m\",\"vel_mps\",\"Altitud\",\"s_m\",\"dist_m\",\n",
    "    \"time_diff\",\"dwell_same_xy_s\",\"ETA_proxima_est_s\"\n",
    "]\n",
    "int_cols = [\"hour\",\"dow\"]\n",
    "boolish_cols = [\"is_no_progress\",\"progress_event\",\"is_weekend\",\"is_peak\"]\n",
    "\n",
    "# Convertir tipos\n",
    "for c in str_cols:\n",
    "    data[c] = data[c].astype(\"category\")\n",
    "for c in dt_cols:\n",
    "    data[c] = pd.to_datetime(data[c])\n",
    "for c in float_cols:\n",
    "    data[c] = data[c].astype(\"float32\")\n",
    "for c in int_cols:\n",
    "    data[c] = data[c].astype(\"int32\")\n",
    "for c in boolish_cols:\n",
    "    data[c] = data[c].astype(\"bool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd74ce5",
   "metadata": {},
   "source": [
    "### Train / valid / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35b8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "dev_df = df[df[\"Fecha\"] < \"2025-04-01\"]   # TODO lo anterior a abril 2025\n",
    "test_df = df[df[\"Fecha\"] >= \"2025-04-01\"] # TODO abril 2025 en adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f326362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_based_time_cv_full(\n",
    "    df: pd.DataFrame,\n",
    "    day_col: str = \"Fecha\",\n",
    "    min_train_days: int = 30,   # entrena al menos 30 días antes del 1er fold\n",
    "    valid_days: int = 2,        # ventana de validación por fold\n",
    "    step_days: int = 7,         # cuánto avanzas la ventana por fold (stride)\n",
    "    embargo_days: int = 0,      # buffer entre train y valid (opcional)\n",
    "    max_splits = None,  # None = hasta agotar el timeline\n",
    ") -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
    "    d = df.copy()\n",
    "    d[\"__day__\"] = pd.to_datetime(d[day_col]).dt.normalize()\n",
    "    unique_days = np.array(sorted(d[\"__day__\"].unique()))\n",
    "    total_days = len(unique_days)\n",
    "\n",
    "    start_valid = min_train_days + embargo_days\n",
    "    splits = 0\n",
    "    while start_valid + valid_days <= total_days:\n",
    "        train_last = start_valid - embargo_days - 1\n",
    "        valid_start = start_valid\n",
    "        valid_end   = start_valid + valid_days  # exclusivo\n",
    "\n",
    "        train_days = set(unique_days[:train_last+1])\n",
    "        valid_days_set = set(unique_days[valid_start:valid_end])\n",
    "\n",
    "        mask_train = d[\"__day__\"].isin(train_days).values\n",
    "        mask_valid = d[\"__day__\"].isin(valid_days_set).values\n",
    "\n",
    "        tr_idx = df.index[mask_train].values\n",
    "        va_idx = df.index[mask_valid].values\n",
    "\n",
    "        yield np.sort(tr_idx), np.sort(va_idx)\n",
    "\n",
    "        splits += 1\n",
    "        if (max_splits is not None) and (splits >= max_splits):\n",
    "            break\n",
    "        start_valid += step_days  # avanza la ventana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56f61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_splits(df, splits, day_col=\"Fecha\", key_cols=(\"Placa\",\"trip_id\")):\n",
    "    df = df.copy()\n",
    "    df[\"__day__\"] = pd.to_datetime(df[day_col]).dt.normalize()\n",
    "    df[\"__trip_key__\"] = list(zip(*[df[c].astype(str) for c in key_cols]))  # (Placa, trip_id)\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(splits, 1):\n",
    "        tr_days = df.loc[tr_idx, \"__day__\"].unique()\n",
    "        va_days = df.loc[va_idx, \"__day__\"].unique()\n",
    "\n",
    "        print(f\"\\nFold {i}\")\n",
    "        print(\"  Train days:\", tr_days.min(), \"→\", tr_days.max(), f\"({len(tr_days)} días, {len(tr_idx):,} filas)\")\n",
    "        print(\"  Valid days:\", va_days.min(), \"→\", va_days.max(), f\"({len(va_days)} días, {len(va_idx):,} filas)\")\n",
    "        day_overlap = set(tr_days) & set(va_days)\n",
    "        print(\"  Day overlap? \", \"YES\" if day_overlap else \"NO\")\n",
    "\n",
    "        tr_keys = set(df.loc[tr_idx, \"__trip_key__\"].unique())\n",
    "        va_keys = set(df.loc[va_idx, \"__trip_key__\"].unique())\n",
    "        key_overlap = tr_keys & va_keys\n",
    "        print(\"  Trip overlap (Placa,trip_id)? \", f\"YES ({len(key_overlap)})\" if key_overlap else \"NO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f313d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-01-30 00:00:00 (30 días, 322,634 filas)\n",
      "  Valid days: 2024-01-31 00:00:00 → 2024-02-04 00:00:00 (5 días, 45,113 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (2)\n",
      "\n",
      "Fold 2\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-02-29 00:00:00 (60 días, 668,305 filas)\n",
      "  Valid days: 2024-03-01 00:00:00 → 2024-03-05 00:00:00 (5 días, 53,119 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (3)\n",
      "\n",
      "Fold 3\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-03-30 00:00:00 (90 días, 962,035 filas)\n",
      "  Valid days: 2024-03-31 00:00:00 → 2024-04-04 00:00:00 (5 días, 52,903 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (4)\n",
      "\n",
      "Fold 4\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-04-29 00:00:00 (120 días, 1,311,520 filas)\n",
      "  Valid days: 2024-04-30 00:00:00 → 2024-05-04 00:00:00 (5 días, 40,338 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (6)\n",
      "\n",
      "Fold 5\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-05-29 00:00:00 (150 días, 1,627,891 filas)\n",
      "  Valid days: 2024-05-30 00:00:00 → 2024-06-03 00:00:00 (5 días, 46,620 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (2)\n",
      "\n",
      "Fold 6\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-06-28 00:00:00 (180 días, 1,982,664 filas)\n",
      "  Valid days: 2024-06-29 00:00:00 → 2024-07-03 00:00:00 (5 días, 51,017 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (1)\n",
      "\n",
      "Fold 7\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-07-28 00:00:00 (210 días, 2,313,902 filas)\n",
      "  Valid days: 2024-07-29 00:00:00 → 2024-08-02 00:00:00 (5 días, 52,497 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (2)\n",
      "\n",
      "Fold 8\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-08-27 00:00:00 (240 días, 2,598,317 filas)\n",
      "  Valid days: 2024-08-28 00:00:00 → 2024-09-01 00:00:00 (5 días, 58,990 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (4)\n",
      "\n",
      "Fold 9\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-09-26 00:00:00 (270 días, 2,919,549 filas)\n",
      "  Valid days: 2024-09-27 00:00:00 → 2024-10-01 00:00:00 (5 días, 55,134 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (2)\n",
      "\n",
      "Fold 10\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-10-26 00:00:00 (300 días, 3,248,659 filas)\n",
      "  Valid days: 2024-10-27 00:00:00 → 2024-10-31 00:00:00 (5 días, 50,392 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (4)\n",
      "\n",
      "Fold 11\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-11-25 00:00:00 (330 días, 3,534,130 filas)\n",
      "  Valid days: 2024-11-26 00:00:00 → 2024-11-30 00:00:00 (5 días, 62,737 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (4)\n",
      "\n",
      "Fold 12\n",
      "  Train days: 2024-01-01 00:00:00 → 2024-12-25 00:00:00 (360 días, 3,845,409 filas)\n",
      "  Valid days: 2024-12-26 00:00:00 → 2024-12-30 00:00:00 (5 días, 43,920 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (4)\n",
      "\n",
      "Fold 13\n",
      "  Train days: 2024-01-01 00:00:00 → 2025-01-24 00:00:00 (390 días, 4,115,106 filas)\n",
      "  Valid days: 2025-01-25 00:00:00 → 2025-01-29 00:00:00 (5 días, 51,015 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (6)\n",
      "\n",
      "Fold 14\n",
      "  Train days: 2024-01-01 00:00:00 → 2025-02-23 00:00:00 (420 días, 4,378,550 filas)\n",
      "  Valid days: 2025-02-24 00:00:00 → 2025-02-28 00:00:00 (5 días, 41,550 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (2)\n",
      "\n",
      "Fold 15\n",
      "  Train days: 2024-01-01 00:00:00 → 2025-03-26 00:00:00 (450 días, 4,657,914 filas)\n",
      "  Valid days: 2025-03-27 00:00:00 → 2025-03-31 00:00:00 (5 días, 50,927 filas)\n",
      "  Day overlap?  NO\n",
      "  Trip overlap (Placa,trip_id)?  YES (3)\n"
     ]
    }
   ],
   "source": [
    "splits = list(day_based_time_cv_full(\n",
    "    dev_df,\n",
    "    day_col=\"Fecha\",\n",
    "    min_train_days=30,\n",
    "    valid_days=5,\n",
    "    step_days=30,        # un fold por dos meses\n",
    "    embargo_days=0,     # buffer de 0 días\n",
    "    max_splits=None     # None = hasta el final del timeline\n",
    "))\n",
    "\n",
    "\n",
    "# Verificar los splits\n",
    "summarize_splits(dev_df, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25da36f",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c840662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir características y objetivo\n",
    "\n",
    "categorical_features = [\"LINEA\",\"DIR\",\"proxima_est_teorica\"]\n",
    "numeric_features = [\n",
    "    \"dist_a_prox_m\",\"dist_estacion_m\",\n",
    "    \"vel_mps\",\"Altitud\",\"s_m\",\"dist_m\",\n",
    "    \"time_diff\",\"dwell_same_xy_s\",\"hour\",\"dow\",\n",
    "    \"is_no_progress\",\"progress_event\",\"is_weekend\",\"is_peak\"\n",
    "]\n",
    "\n",
    "feature_cols = categorical_features + numeric_features\n",
    "target_col = \"ETA_proxima_est_s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f886946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6640f",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dev_df[feature_cols]\n",
    "y_train = dev_df[target_col]\n",
    "X_valid = test_df[feature_cols]\n",
    "y_valid = test_df[target_col]\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a310239",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Entrenar modelo\u001b[39;00m\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     19\u001b[0m     params,\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mtrain_data\u001b[49m,\n\u001b[0;32m     21\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39m[train_data, valid_data],\n\u001b[0;32m     22\u001b[0m     valid_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     23\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m,\n\u001b[0;32m     24\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     25\u001b[0m         lgb\u001b[38;5;241m.\u001b[39mearly_stopping(stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     26\u001b[0m         lgb\u001b[38;5;241m.\u001b[39mlog_evaluation(period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m),\n\u001b[0;32m     27\u001b[0m     ]\n\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Definir parámetros\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": [\"mae\",\"rmse\"],\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"max_bin\": 255,\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"verbose\": 1\n",
    "}\n",
    "\n",
    "# Entrenar modelo\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=[\"train\",\"valid\"],\n",
    "    num_boost_round=3000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=300, verbose=True),\n",
    "        lgb.log_evaluation(period=100),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f28e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linea: Linea_12, Siguiente estación: CENMA, Predicción: 328.60, Real: 600.00\n",
      "Linea: Linea_12, Siguiente estación: CENMA, Predicción: 589.14, Real: 300.00\n",
      "Linea: Linea_12, Siguiente estación: TRÉBOL DIRECCIÓN CENTRO, Predicción: 382.93, Real: 600.00\n",
      "Linea: Linea_12, Siguiente estación: TRÉBOL DIRECCIÓN CENTRO, Predicción: 584.19, Real: 300.00\n",
      "Linea: Linea_12, Siguiente estación: LAS CHARCAS DIRECCIÓN CENMA, Predicción: 362.56, Real: 300.00\n"
     ]
    }
   ],
   "source": [
    "# Realizar una predicción de prueba\n",
    "for i in range(5):\n",
    "    print(f'Linea: {X_test[\"LINEA\"].iloc[i]}, Siguiente estación: {X_test[\"proxima_est_teorica\"].iloc[i]}, Predicción: {y_pred[i]:.2f}, Real: {y_test.iloc[i]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f681c50",
   "metadata": {},
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a666e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 125.70 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"MAE: {mae:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743051a",
   "metadata": {},
   "source": [
    "RSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 351.43 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"RMSE: {rmse:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9fe381",
   "metadata": {},
   "source": [
    "R2 - Coeficiente de determinación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb1e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1413630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo Zamora\\AppData\\Local\\Temp\\ipykernel_28408\\508538639.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  line_stats = by_line.groupby(\"LINEA\")[\"abs_err\"].agg([\"mean\",\"median\",\"count\"]).sort_values(\"mean\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  mean      median    count\n",
      "LINEA                                      \n",
      "Linea_1      61.126804   32.019461   577277\n",
      "Linea_13-A   94.980999   40.152969   371452\n",
      "Linea_6     104.827698   40.983663   939870\n",
      "Linea_12    105.028820   28.941709  1772805\n",
      "Linea_2     112.369283   49.643148   239408\n",
      "Linea_7     152.022106   58.874034    37876\n",
      "Linea_18-A  167.657788   79.098083   806776\n",
      "Linea_18-B  181.953693   87.914202   170019\n",
      "Linea_13-B  212.699097  103.227633    13042\n"
     ]
    }
   ],
   "source": [
    "by_line = data.copy()\n",
    "by_line[\"pred\"] = model.predict(X, num_iteration=model.best_iteration)\n",
    "by_line[\"abs_err\"] = np.abs(by_line[\"pred\"] - by_line[\"ETA_proxima_est_s\"])\n",
    "line_stats = by_line.groupby(\"LINEA\", observed=False)[\"abs_err\"].agg([\"mean\",\"median\",\"count\"]).sort_values(\"mean\")\n",
    "print('MAE por línea:')\n",
    "print(line_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbafb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x29567e01910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Guardar modelo\n",
    "model.save_model(\"lightgbm_baseline_model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f310b49",
   "metadata": {},
   "source": [
    "Validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e344efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para métricas\n",
    "\n",
    "SLA_THRESH = [60, 120, 180]    # segundos\n",
    "\n",
    "def compute_metrics(y_true, y_pred, sla_thresh=SLA_THRESH):\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(root_mean_squared_error(y_true, y_pred))\n",
    "    abs_err = np.abs(y_pred - y_true)\n",
    "    sla = {f\"sla_le_{t}s\": float((abs_err <= t).mean()) for t in sla_thresh}\n",
    "    return {\"mae\": mae, \"rmse\": rmse, **sla}\n",
    "\n",
    "\n",
    "# Definir parámetros\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": [\"mae\",\"rmse\"],\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"max_bin\": 255,\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"verbose\": 1,\n",
    "    \"seed\": SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b543ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2042\n",
      "[LightGBM] [Info] Number of data points in the train set: 322634, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 275.678341\n",
      "\n",
      "=== Fold 2 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2043\n",
      "[LightGBM] [Info] Number of data points in the train set: 668305, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 274.850839\n",
      "\n",
      "=== Fold 3 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2043\n",
      "[LightGBM] [Info] Number of data points in the train set: 962035, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 275.814519\n",
      "\n",
      "=== Fold 4 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2060\n",
      "[LightGBM] [Info] Number of data points in the train set: 1311520, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 275.618378\n",
      "\n",
      "=== Fold 5 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2060\n",
      "[LightGBM] [Info] Number of data points in the train set: 1627891, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 274.651375\n",
      "\n",
      "=== Fold 6 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1978\n",
      "[LightGBM] [Info] Number of data points in the train set: 1982664, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 276.682127\n",
      "\n",
      "=== Fold 7 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2057\n",
      "[LightGBM] [Info] Number of data points in the train set: 2313902, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 277.587536\n",
      "\n",
      "=== Fold 8 ===\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.361064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 2598317, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 278.317083\n",
      "\n",
      "=== Fold 9 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2115\n",
      "[LightGBM] [Info] Number of data points in the train set: 2919549, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 280.206524\n",
      "\n",
      "=== Fold 10 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2108\n",
      "[LightGBM] [Info] Number of data points in the train set: 3248659, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 281.409284\n",
      "\n",
      "=== Fold 11 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2120\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534130, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 281.481270\n",
      "\n",
      "=== Fold 12 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2108\n",
      "[LightGBM] [Info] Number of data points in the train set: 3845409, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 281.854152\n",
      "\n",
      "=== Fold 13 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2111\n",
      "[LightGBM] [Info] Number of data points in the train set: 4115106, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 281.749426\n",
      "\n",
      "=== Fold 14 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2119\n",
      "[LightGBM] [Info] Number of data points in the train set: 4378550, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 281.733728\n",
      "\n",
      "=== Fold 15 ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2106\n",
      "[LightGBM] [Info] Number of data points in the train set: 4657914, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 281.946611\n"
     ]
    }
   ],
   "source": [
    "fold_results = []\n",
    "models = []\n",
    "best_iters = []\n",
    "fi_gain_list = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(splits, 1):\n",
    "    \n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    \n",
    "    X_train = dev_df.loc[tr_idx, feature_cols]\n",
    "    y_train = dev_df.loc[tr_idx, target_col]\n",
    "    X_valid = dev_df.loc[va_idx, feature_cols]\n",
    "    y_valid = dev_df.loc[va_idx, target_col]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features, free_raw_data=True)\n",
    "    lgb_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, free_raw_data=True)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_valid],\n",
    "        valid_names=[f\"valid{fold}\"],\n",
    "        num_boost_round=3000,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    metrics = compute_metrics(y_valid.values, y_pred, SLA_THRESH)\n",
    "    metrics[\"fold\"] = fold\n",
    "    metrics[\"best_iter\"] = int(model.best_iteration)\n",
    "    fold_results.append(metrics)\n",
    "    best_iters.append(model.best_iteration)\n",
    "    models.append(model)\n",
    "\n",
    "    # importancias (gain)\n",
    "    fi_gain = pd.DataFrame({\n",
    "        \"feature\": feature_cols,\n",
    "        \"gain\": model.feature_importance(importance_type=\"gain\"),\n",
    "        \"fold\": fold,\n",
    "    })\n",
    "    fi_gain_list.append(fi_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "859e81e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas por fold ===\n",
      "             mae        rmse  sla_le_60s  sla_le_120s  sla_le_180s  best_iter\n",
      "fold                                                                         \n",
      "1     110.239148  305.263848    0.640591     0.793917     0.861570        515\n",
      "2      89.619609  202.398015    0.640317     0.811367     0.880024        313\n",
      "3     107.678575  322.144033    0.647752     0.809198     0.875206        742\n",
      "4     105.644139  291.310490    0.650206     0.805270     0.870023       1046\n",
      "5      90.556050  242.656765    0.667653     0.822287     0.887538       1152\n",
      "6      97.643400  319.963239    0.685164     0.836701     0.895897        582\n",
      "7      98.614714  281.116513    0.665371     0.819914     0.881250       3000\n",
      "8     114.477586  287.500322    0.610239     0.773504     0.848805        994\n",
      "9     134.672916  335.105836    0.574890     0.742899     0.821036       1165\n",
      "10    135.344141  366.560378    0.612161     0.768495     0.836661       1685\n",
      "11    116.476928  332.094161    0.607281     0.778201     0.854902        785\n",
      "12    110.959834  291.120752    0.615642     0.784085     0.859927        614\n",
      "13     98.516670  267.783600    0.648221     0.809781     0.877997        896\n",
      "14    138.795406  414.966474    0.612539     0.771191     0.842720       2148\n",
      "15    102.880387  270.551420    0.640073     0.799654     0.868635        745\n",
      "\n",
      "=== Promedio CV ± std ===\n",
      "             mae        rmse  sla_le_60s  sla_le_120s  sla_le_180s  \\\n",
      "mean  110.141300  302.035723    0.634540     0.795098     0.864146   \n",
      "std    15.642406   50.815598    0.028724     0.025025     0.020506   \n",
      "\n",
      "        best_iter  \n",
      "mean  1092.133333  \n",
      "std    703.655036  \n",
      "\n",
      "Iteraciones promedio (best_iteration): 1092\n",
      "\n",
      "Top-20 features por gain promedio:\n",
      "feature\n",
      "dist_m                 6.961281e+11\n",
      "proxima_est_teorica    5.841739e+11\n",
      "dist_a_prox_m          4.470127e+11\n",
      "s_m                    3.799560e+11\n",
      "hour                   3.205384e+11\n",
      "dist_estacion_m        2.285399e+11\n",
      "Altitud                1.356026e+11\n",
      "vel_mps                1.127900e+11\n",
      "dow                    8.017715e+10\n",
      "dwell_same_xy_s        7.893091e+10\n",
      "LINEA                  7.793338e+10\n",
      "time_diff              4.093565e+10\n",
      "is_peak                3.284711e+10\n",
      "is_weekend             8.203046e+09\n",
      "DIR                    7.317942e+09\n",
      "progress_event         4.513432e+09\n",
      "is_no_progress         2.079764e+09\n",
      "Name: gain, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(fold_results).set_index(\"fold\")\n",
    "print(\"\\n=== Métricas por fold ===\")\n",
    "print(cv_df)\n",
    "\n",
    "print(\"\\n=== Promedio CV ± std ===\")\n",
    "summary = cv_df.agg([\"mean\",\"std\"])\n",
    "print(summary)\n",
    "\n",
    "avg_best_iter = int(np.mean(best_iters))\n",
    "print(f\"\\nIteraciones promedio (best_iteration): {avg_best_iter}\")\n",
    "\n",
    "fi_gain_all = pd.concat(fi_gain_list, ignore_index=True)\n",
    "fi_gain_mean = fi_gain_all.groupby(\"feature\")[\"gain\"].mean().sort_values(ascending=False)\n",
    "print(\"\\nTop-20 features por gain promedio:\")\n",
    "print(fi_gain_mean.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db62405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelos entrenados por fold\n",
    "import os\n",
    "\n",
    "model_dir = \"D:/2025/UVG/Tesis/repos/backend/models/lightgbm/cross_validation_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for fold, model in enumerate(models, 1):\n",
    "    model.save_model(f\"{model_dir}/lgb_model_fold{fold}.txt\")\n",
    "    \n",
    "# Guardar métricas CV\n",
    "cv_df.to_csv(f\"{model_dir}/cv_metrics.csv\")\n",
    "# Guardar importancias\n",
    "fi_gain_all.to_csv(f\"{model_dir}/cv_feature_importances_gain.csv\", index=False)\n",
    "\n",
    "# Guardar parámetros usados\n",
    "import json\n",
    "with open(f\"{model_dir}/lgb_params.json\", \"w\") as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab19fa6",
   "metadata": {},
   "source": [
    "Modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1e0c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.276427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2115\n",
      "[LightGBM] [Info] Number of data points in the train set: 4708841, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 281.770815\n"
     ]
    }
   ],
   "source": [
    "final_rounds = int(np.median(best_iters))\n",
    "\n",
    "lgb_train_full = lgb.Dataset(dev_df[feature_cols], label=dev_df[target_col], categorical_feature=categorical_features)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    {**params, \"metric\": []},\n",
    "    lgb_train_full,\n",
    "    num_boost_round=final_rounds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5743a24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas en set de prueba (LightGBM final) ===\n",
      "mae: 122.41 s\n",
      "rmse: 344.11 s\n",
      "sla_le_60s: 60.63%\n",
      "sla_le_120s: 77.09%\n",
      "sla_le_180s: 84.66%\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "# Métricas en set de prueba\n",
    "y_test_pred = final_model.predict(test_df[feature_cols], num_iteration=final_model.best_iteration)\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "test_metrics = compute_metrics(y_test.values, y_test_pred, SLA_THRESH)\n",
    "print(\"\\n=== Métricas en set de prueba (LightGBM final) ===\")\n",
    "\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v*100:.2f}%\" if k.startswith(\"sla_\") else f\"{k}: {v:.2f} s\")\n",
    "    \n",
    "# Guardar modelo final\n",
    "final_dir = \"D:/2025/UVG/Tesis/repos/backend/models/lightgbm/final\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "final_model.save_model(f\"{final_dir}/lgb_final_model.txt\")\n",
    "\n",
    "# Guardar métricas del set de prueba\n",
    "with open(f\"{final_dir}/test_metrics.json\", \"w\") as f:\n",
    "    json.dump({k: float(v) for k, v in test_metrics.items()}, f, indent=2)\n",
    "    \n",
    "# Guardar parámetros y metadatos del modelo final\n",
    "\n",
    "final_meta = {\n",
    "    \"final_rounds\": final_rounds,\n",
    "    \"params\": params,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"target_col\": target_col,\n",
    "}\n",
    "\n",
    "with open(f\"{final_dir}/final_meta.json\", \"w\") as f:\n",
    "    json.dump(final_meta, f, indent=4)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"Fecha\": test_df[\"Fecha\"].values,\n",
    "    \"Placa\": test_df[\"Placa\"].astype(str).values if \"Placa\" in test_df.columns else None,\n",
    "    \"ETA_true\": test_df[target_col].values,\n",
    "    \"ETA_pred\": y_test_pred,\n",
    "    \"abs_error\": np.abs(test_df[target_col].values - y_test_pred)\n",
    "}).to_csv(f\"{final_dir}/test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c296a6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST por LINEA,DIR ===\n",
      "         LINEA       DIR         MAE        RMSE   SLA<=60  SLA<=120  \\\n",
      "5     Linea_12    VUELTA   39.570690  172.728891  0.858160  0.969994   \n",
      "7   Linea_13-A       IDA   61.148827  115.979566  0.706454  0.877869   \n",
      "26     Linea_7    VUELTA   74.618699  189.282244  0.708111  0.860791   \n",
      "0      Linea_1  CIRCULAR   79.446953  285.779571  0.705392  0.875254   \n",
      "23     Linea_6    VUELTA   79.451076  224.428435  0.686085  0.850472   \n",
      "8   Linea_13-A    VUELTA   80.054807  214.688917  0.697359  0.848198   \n",
      "16  Linea_18-B       IDA   80.105929  146.404522  0.616710  0.809240   \n",
      "25     Linea_7       IDA  106.358448  320.592524  0.606039  0.815849   \n",
      "10  Linea_13-B       IDA  115.988058  249.128305  0.550382  0.750382   \n",
      "19     Linea_2       IDA  121.815585  372.686242  0.517793  0.719994   \n",
      "\n",
      "    SLA<=180        n  \n",
      "5   0.990278  34560.0  \n",
      "7   0.937873  27798.0  \n",
      "26  0.920853   3563.0  \n",
      "0   0.930373  25091.0  \n",
      "23  0.912500  12720.0  \n",
      "8   0.903372  30105.0  \n",
      "16  0.891178   6212.0  \n",
      "25  0.887764   5729.0  \n",
      "10  0.843511   1310.0  \n",
      "19  0.838812   6632.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo Zamora\\AppData\\Local\\Temp\\ipykernel_34668\\3329905506.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  agg = tmp.groupby(by).apply(\n"
     ]
    }
   ],
   "source": [
    "# Desglose por línea\n",
    "def group_report(df, y_true, y_pred, by=[\"LINEA\",\"DIR\"]):\n",
    "    tmp = df.copy()\n",
    "    tmp[\"y_true\"] = y_true\n",
    "    tmp[\"y_pred\"] = y_pred\n",
    "    tmp[\"abs_err\"] = (y_true - y_pred).abs()\n",
    "    agg = tmp.groupby(by).apply(\n",
    "        lambda g: pd.Series({\n",
    "            \"MAE\": g[\"abs_err\"].mean(),\n",
    "            \"RMSE\": (( (g[\"y_true\"]-g[\"y_pred\"])**2 ).mean())**0.5,\n",
    "            \"SLA<=60\": (g[\"abs_err\"]<=60).mean(),\n",
    "            \"SLA<=120\": (g[\"abs_err\"]<=120).mean(),\n",
    "            \"SLA<=180\": (g[\"abs_err\"]<=180).mean(),\n",
    "            \"n\": len(g)\n",
    "        })\n",
    "    ).reset_index()\n",
    "    return agg\n",
    "\n",
    "seg = group_report(test_df, test_df[target_col], y_test_pred, by=[\"LINEA\",\"DIR\"])\n",
    "print(\"\\n=== TEST por LINEA,DIR ===\")\n",
    "print(seg.sort_values(\"MAE\").head(10))\n",
    "seg.to_csv(f\"{final_dir}/test_segment_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
